{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67ebc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1c43e",
   "metadata": {},
   "source": [
    "## Multi Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa184f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>search_word</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>blog_name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>content_len</th>\n",
       "      <th>content_hash_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>e_sum_758</th>\n",
       "      <th>e_sum_759</th>\n",
       "      <th>e_sum_760</th>\n",
       "      <th>e_sum_761</th>\n",
       "      <th>e_sum_762</th>\n",
       "      <th>e_sum_763</th>\n",
       "      <th>e_sum_764</th>\n",
       "      <th>e_sum_765</th>\n",
       "      <th>e_sum_766</th>\n",
       "      <th>e_sum_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>shuenmama.223027650182</td>\n",
       "      <td>홍대 회식 맛집</td>\n",
       "      <td>홍대 고기집 데이트 쟁반한상 삼겹살 맛집 회식 쟁반집8292</td>\n",
       "      <td>https://blog.naver.com/shuenmama/223027650182</td>\n",
       "      <td>shuen</td>\n",
       "      <td>20230225</td>\n",
       "      <td>홍대에서 데이트 하기로 한 주말\\n진짜 간만에 홍대\\n전에 항상 세우던 공영주차장이...</td>\n",
       "      <td>3094</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226764</td>\n",
       "      <td>-0.341097</td>\n",
       "      <td>0.107354</td>\n",
       "      <td>0.395617</td>\n",
       "      <td>-0.184648</td>\n",
       "      <td>-0.187328</td>\n",
       "      <td>-0.100806</td>\n",
       "      <td>0.066111</td>\n",
       "      <td>0.315414</td>\n",
       "      <td>-0.584649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rosepink1974.223153722255</td>\n",
       "      <td>홍대 회식 맛집</td>\n",
       "      <td>홍대삼겹살 청년화로 1987 이베리코 연남동고기집 회식맛집</td>\n",
       "      <td>https://blog.naver.com/rosepink1974/223153722255</td>\n",
       "      <td>예쁜 달코미의 단맛 인생</td>\n",
       "      <td>20230712</td>\n",
       "      <td>청년화로1987\\n서울 마포구 동교로 219 1층\\n청년화로 1987\\n홍대입구역 ...</td>\n",
       "      <td>2345</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274786</td>\n",
       "      <td>-0.311202</td>\n",
       "      <td>-0.107873</td>\n",
       "      <td>0.219888</td>\n",
       "      <td>-0.324829</td>\n",
       "      <td>-0.479143</td>\n",
       "      <td>-0.085153</td>\n",
       "      <td>-0.008704</td>\n",
       "      <td>0.342880</td>\n",
       "      <td>-0.252228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mou25.223209216526</td>\n",
       "      <td>홍대 회식 맛집</td>\n",
       "      <td>합정 맛집 홍대 회식장소로 딱, 느낌 있는 소고기 고깃집...</td>\n",
       "      <td>https://blog.naver.com/mou25/223209216526</td>\n",
       "      <td>생애 기록장</td>\n",
       "      <td>20230912</td>\n",
       "      <td>매번 느끼는 거지만,\\n회식장소 하나는\\n기가 막히게 섭외하는 울 주임님.\\n\\n얼...</td>\n",
       "      <td>2904</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297851</td>\n",
       "      <td>0.366678</td>\n",
       "      <td>0.195712</td>\n",
       "      <td>0.164166</td>\n",
       "      <td>0.123258</td>\n",
       "      <td>-0.229331</td>\n",
       "      <td>0.080041</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.381098</td>\n",
       "      <td>-0.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>lulu_l.223118434610</td>\n",
       "      <td>홍대 회식 맛집</td>\n",
       "      <td>홍대 맛집 합정 갈비가 부드러운 소고기집 연막탄 회식, 데이트...</td>\n",
       "      <td>https://blog.naver.com/lulu_l/223118434610</td>\n",
       "      <td>안나의 일상공유</td>\n",
       "      <td>20230602</td>\n",
       "      <td>홍대 소고기 맛집\\n연막탄\\n\\n남자친구가 맛있는 고깃집을 알고\\n있다길해 합정 맛...</td>\n",
       "      <td>2810</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297081</td>\n",
       "      <td>-0.099999</td>\n",
       "      <td>0.227369</td>\n",
       "      <td>-0.031295</td>\n",
       "      <td>0.096868</td>\n",
       "      <td>-0.391839</td>\n",
       "      <td>-0.211752</td>\n",
       "      <td>0.156410</td>\n",
       "      <td>-0.170814</td>\n",
       "      <td>-0.501321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ruston_.223161590597</td>\n",
       "      <td>홍대 회식 맛집</td>\n",
       "      <td>홍대회식, 육즙 폭발하는 소고기 맛집 '일편등심 홍대본점'</td>\n",
       "      <td>https://blog.naver.com/ruston_/223161590597</td>\n",
       "      <td>로빈이 토끼란 사실을 알고있었나?</td>\n",
       "      <td>20230720</td>\n",
       "      <td>안녕하세요. LoLCake입니다.\\n\\n\\n최근 이직을 준비하는 동료의 축하 파티를...</td>\n",
       "      <td>2299</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405006</td>\n",
       "      <td>-0.211296</td>\n",
       "      <td>0.137026</td>\n",
       "      <td>0.264585</td>\n",
       "      <td>-0.045813</td>\n",
       "      <td>-0.435813</td>\n",
       "      <td>-0.275717</td>\n",
       "      <td>-0.078445</td>\n",
       "      <td>0.584033</td>\n",
       "      <td>-0.496703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 799 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                         id search_word  \\\n",
       "0      0     shuenmama.223027650182    홍대 회식 맛집   \n",
       "1      1  rosepink1974.223153722255    홍대 회식 맛집   \n",
       "2      2         mou25.223209216526    홍대 회식 맛집   \n",
       "3      3        lulu_l.223118434610    홍대 회식 맛집   \n",
       "4      4       ruston_.223161590597    홍대 회식 맛집   \n",
       "\n",
       "                                    title  \\\n",
       "0       홍대 고기집 데이트 쟁반한상 삼겹살 맛집 회식 쟁반집8292   \n",
       "1        홍대삼겹살 청년화로 1987 이베리코 연남동고기집 회식맛집   \n",
       "2     합정 맛집 홍대 회식장소로 딱, 느낌 있는 소고기 고깃집...    \n",
       "3  홍대 맛집 합정 갈비가 부드러운 소고기집 연막탄 회식, 데이트...    \n",
       "4        홍대회식, 육즙 폭발하는 소고기 맛집 '일편등심 홍대본점'   \n",
       "\n",
       "                                                url           blog_name  \\\n",
       "0     https://blog.naver.com/shuenmama/223027650182               shuen   \n",
       "1  https://blog.naver.com/rosepink1974/223153722255       예쁜 달코미의 단맛 인생   \n",
       "2         https://blog.naver.com/mou25/223209216526              생애 기록장   \n",
       "3        https://blog.naver.com/lulu_l/223118434610            안나의 일상공유   \n",
       "4       https://blog.naver.com/ruston_/223161590597  로빈이 토끼란 사실을 알고있었나?   \n",
       "\n",
       "       date                                            content  content_len  \\\n",
       "0  20230225  홍대에서 데이트 하기로 한 주말\\n진짜 간만에 홍대\\n전에 항상 세우던 공영주차장이...         3094   \n",
       "1  20230712  청년화로1987\\n서울 마포구 동교로 219 1층\\n청년화로 1987\\n홍대입구역 ...         2345   \n",
       "2  20230912  매번 느끼는 거지만,\\n회식장소 하나는\\n기가 막히게 섭외하는 울 주임님.\\n\\n얼...         2904   \n",
       "3  20230602  홍대 소고기 맛집\\n연막탄\\n\\n남자친구가 맛있는 고깃집을 알고\\n있다길해 합정 맛...         2810   \n",
       "4  20230720  안녕하세요. LoLCake입니다.\\n\\n\\n최근 이직을 준비하는 동료의 축하 파티를...         2299   \n",
       "\n",
       "   content_hash_cnt  ...  e_sum_758  e_sum_759  e_sum_760  e_sum_761  \\\n",
       "0                10  ...   0.226764  -0.341097   0.107354   0.395617   \n",
       "1                 9  ...   0.274786  -0.311202  -0.107873   0.219888   \n",
       "2                12  ...   0.297851   0.366678   0.195712   0.164166   \n",
       "3                 0  ...   0.297081  -0.099999   0.227369  -0.031295   \n",
       "4                 2  ...   0.405006  -0.211296   0.137026   0.264585   \n",
       "\n",
       "   e_sum_762  e_sum_763  e_sum_764  e_sum_765  e_sum_766  e_sum_767  \n",
       "0  -0.184648  -0.187328  -0.100806   0.066111   0.315414  -0.584649  \n",
       "1  -0.324829  -0.479143  -0.085153  -0.008704   0.342880  -0.252228  \n",
       "2   0.123258  -0.229331   0.080041   0.002673   0.381098  -0.208800  \n",
       "3   0.096868  -0.391839  -0.211752   0.156410  -0.170814  -0.501321  \n",
       "4  -0.045813  -0.435813  -0.275717  -0.078445   0.584033  -0.496703  \n",
       "\n",
       "[5 rows x 799 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/review_dataset_v6_sum_e.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da52db59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>e_sum_0</th>\n",
       "      <th>e_sum_1</th>\n",
       "      <th>e_sum_2</th>\n",
       "      <th>e_sum_3</th>\n",
       "      <th>e_sum_4</th>\n",
       "      <th>e_sum_5</th>\n",
       "      <th>e_sum_6</th>\n",
       "      <th>e_sum_7</th>\n",
       "      <th>e_sum_8</th>\n",
       "      <th>...</th>\n",
       "      <th>e_sum_758</th>\n",
       "      <th>e_sum_759</th>\n",
       "      <th>e_sum_760</th>\n",
       "      <th>e_sum_761</th>\n",
       "      <th>e_sum_762</th>\n",
       "      <th>e_sum_763</th>\n",
       "      <th>e_sum_764</th>\n",
       "      <th>e_sum_765</th>\n",
       "      <th>e_sum_766</th>\n",
       "      <th>e_sum_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shuenmama.223027650182</td>\n",
       "      <td>-0.269514</td>\n",
       "      <td>-0.339220</td>\n",
       "      <td>-0.330182</td>\n",
       "      <td>0.040266</td>\n",
       "      <td>0.184372</td>\n",
       "      <td>-0.227383</td>\n",
       "      <td>0.265280</td>\n",
       "      <td>0.116666</td>\n",
       "      <td>-0.206969</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178983</td>\n",
       "      <td>-0.214031</td>\n",
       "      <td>-0.059638</td>\n",
       "      <td>0.167407</td>\n",
       "      <td>0.020884</td>\n",
       "      <td>-0.203415</td>\n",
       "      <td>-0.161502</td>\n",
       "      <td>0.018985</td>\n",
       "      <td>0.237754</td>\n",
       "      <td>-0.221653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rosepink1974.223153722255</td>\n",
       "      <td>-0.187719</td>\n",
       "      <td>-0.324055</td>\n",
       "      <td>-0.297384</td>\n",
       "      <td>0.109595</td>\n",
       "      <td>0.266376</td>\n",
       "      <td>-0.113839</td>\n",
       "      <td>0.198577</td>\n",
       "      <td>0.275900</td>\n",
       "      <td>0.041351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254710</td>\n",
       "      <td>-0.104253</td>\n",
       "      <td>-0.208398</td>\n",
       "      <td>0.202277</td>\n",
       "      <td>-0.376773</td>\n",
       "      <td>-0.346854</td>\n",
       "      <td>-0.018586</td>\n",
       "      <td>0.084422</td>\n",
       "      <td>0.438359</td>\n",
       "      <td>-0.133161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mou25.223209216526</td>\n",
       "      <td>0.084926</td>\n",
       "      <td>-0.473558</td>\n",
       "      <td>-0.147312</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.088285</td>\n",
       "      <td>-0.097935</td>\n",
       "      <td>-0.042144</td>\n",
       "      <td>0.203255</td>\n",
       "      <td>-0.180874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370275</td>\n",
       "      <td>-0.025442</td>\n",
       "      <td>0.027081</td>\n",
       "      <td>0.189991</td>\n",
       "      <td>0.086750</td>\n",
       "      <td>-0.290567</td>\n",
       "      <td>-0.198043</td>\n",
       "      <td>0.072573</td>\n",
       "      <td>0.350379</td>\n",
       "      <td>-0.047624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lulu_l.223118434610</td>\n",
       "      <td>0.049920</td>\n",
       "      <td>-0.534174</td>\n",
       "      <td>-0.485959</td>\n",
       "      <td>0.119297</td>\n",
       "      <td>0.101789</td>\n",
       "      <td>-0.176171</td>\n",
       "      <td>-0.035862</td>\n",
       "      <td>0.220996</td>\n",
       "      <td>0.050428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311148</td>\n",
       "      <td>-0.040022</td>\n",
       "      <td>-0.043510</td>\n",
       "      <td>0.173019</td>\n",
       "      <td>-0.055459</td>\n",
       "      <td>-0.228064</td>\n",
       "      <td>-0.176389</td>\n",
       "      <td>0.105344</td>\n",
       "      <td>0.209823</td>\n",
       "      <td>-0.166638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ruston_.223161590597</td>\n",
       "      <td>-0.102667</td>\n",
       "      <td>-0.188301</td>\n",
       "      <td>-0.230945</td>\n",
       "      <td>-0.044723</td>\n",
       "      <td>0.262243</td>\n",
       "      <td>-0.216912</td>\n",
       "      <td>0.018869</td>\n",
       "      <td>0.162123</td>\n",
       "      <td>0.149256</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318246</td>\n",
       "      <td>0.044601</td>\n",
       "      <td>-0.016627</td>\n",
       "      <td>0.203433</td>\n",
       "      <td>-0.263383</td>\n",
       "      <td>-0.387199</td>\n",
       "      <td>-0.078136</td>\n",
       "      <td>-0.004678</td>\n",
       "      <td>0.465537</td>\n",
       "      <td>-0.184052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       index   e_sum_0   e_sum_1   e_sum_2   e_sum_3  \\\n",
       "0     shuenmama.223027650182 -0.269514 -0.339220 -0.330182  0.040266   \n",
       "1  rosepink1974.223153722255 -0.187719 -0.324055 -0.297384  0.109595   \n",
       "2         mou25.223209216526  0.084926 -0.473558 -0.147312  0.005009   \n",
       "3        lulu_l.223118434610  0.049920 -0.534174 -0.485959  0.119297   \n",
       "4       ruston_.223161590597 -0.102667 -0.188301 -0.230945 -0.044723   \n",
       "\n",
       "    e_sum_4   e_sum_5   e_sum_6   e_sum_7   e_sum_8  ...  e_sum_758  \\\n",
       "0  0.184372 -0.227383  0.265280  0.116666 -0.206969  ...   0.178983   \n",
       "1  0.266376 -0.113839  0.198577  0.275900  0.041351  ...   0.254710   \n",
       "2  0.088285 -0.097935 -0.042144  0.203255 -0.180874  ...   0.370275   \n",
       "3  0.101789 -0.176171 -0.035862  0.220996  0.050428  ...   0.311148   \n",
       "4  0.262243 -0.216912  0.018869  0.162123  0.149256  ...   0.318246   \n",
       "\n",
       "   e_sum_759  e_sum_760  e_sum_761  e_sum_762  e_sum_763  e_sum_764  \\\n",
       "0  -0.214031  -0.059638   0.167407   0.020884  -0.203415  -0.161502   \n",
       "1  -0.104253  -0.208398   0.202277  -0.376773  -0.346854  -0.018586   \n",
       "2  -0.025442   0.027081   0.189991   0.086750  -0.290567  -0.198043   \n",
       "3  -0.040022  -0.043510   0.173019  -0.055459  -0.228064  -0.176389   \n",
       "4   0.044601  -0.016627   0.203433  -0.263383  -0.387199  -0.078136   \n",
       "\n",
       "   e_sum_765  e_sum_766  e_sum_767  \n",
       "0   0.018985   0.237754  -0.221653  \n",
       "1   0.084422   0.438359  -0.133161  \n",
       "2   0.072573   0.350379  -0.047624  \n",
       "3   0.105344   0.209823  -0.166638  \n",
       "4  -0.004678   0.465537  -0.184052  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('./data/review_dataset_embedding_all_mean.csv', index_col=0)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "979cb18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_col = ['adpost_yn', 'map_yn', 'video_yn', 'phone_yn']\n",
    "numerical_col = ['content_len', 'content_hash_cnt', 'article_hash_cnt', 'like_cnt', 'emoticon_cnt', \n",
    "                 'total_post', 'link_cnt', 'image_cnt', 'repeat_word_cnt', 'noun_verb_ratio']\n",
    "embedding_col = ['content']\n",
    "label_col = 'label_f2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "507f97d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.noun_verb_ratio < np.inf].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba8578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    df.set_index('id'),\n",
    "    df2.set_index('index'),\n",
    "    left_index=True, right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d53b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = df['id'].tolist()\n",
    "X = df[categorical_col + numerical_col + embedding_col]\n",
    "y = df[label_col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3fba352",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb15d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X(X):\n",
    "    return (X[categorical_col + numerical_col].values, X[embedding_col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f4e2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat, X_train_emb = split_X(X_train)\n",
    "X_test_feat, X_test_emb = split_X(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa656ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c30bd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeReviewDataset(Dataset):\n",
    "    def __init__(self, feature, embedding, label):\n",
    "        super().__init__()\n",
    "        self.feature = torch.FloatTensor(feature)\n",
    "        self.embedding = embedding.tolist()\n",
    "        self.label = torch.tensor(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"feature\": self.feature[idx],\n",
    "            \"embedding\": self.embedding[idx],\n",
    "            \"label\": self.label[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78a82ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FakeReviewDataset(X_train_feat, X_train_emb, y_train)\n",
    "valid_ds = FakeReviewDataset(X_test_feat, X_test_emb, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28e9a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoader:\n",
    "    def __init__(self, dataset, batch_size: int = 64):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \n",
    "    def get_batch(self, iteration):\n",
    "        idx = range(iteration * self.batch_size, iteration * self.batch_size + self.batch_size)\n",
    "        \n",
    "        datasets = {\n",
    "            'feature': [],\n",
    "            'embedding': [],\n",
    "            'label': []\n",
    "        }\n",
    "        \n",
    "        for i in idx:\n",
    "            try:\n",
    "                pop = self.dataset[i]\n",
    "                for key in datasets.keys():\n",
    "                    datasets[key].append(pop[key])\n",
    "            except:\n",
    "                return {\n",
    "                    \"feature\": torch.tensor([]),\n",
    "                    \"embedding\": [],\n",
    "                    \"label\": torch.tensor([])\n",
    "                }\n",
    "                \n",
    "        datasets['feature'] = torch.vstack(datasets['feature'])\n",
    "        datasets['embedding'] = sum(datasets['embedding'], [])\n",
    "        datasets['label'] = torch.vstack(datasets['label'])\n",
    "        \n",
    "        return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d53f7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=14,\n",
    "        num_layers=3,\n",
    "        step=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        for i in range(10):\n",
    "            if (2 ** i) <= input_size <= (2 ** (i+1)):\n",
    "                init_size = max(2**i, 2**(i+1))\n",
    "                break\n",
    "        \n",
    "        layers = [\n",
    "            nn.Linear(input_size, init_size),\n",
    "            nn.BatchNorm1d(init_size),\n",
    "        ]\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if i < num_layers:\n",
    "                layers.append(nn.ReLU())\n",
    "            \n",
    "            layers.append(nn.Linear(init_size, init_size * (2**step)))\n",
    "            \n",
    "            init_size *= (2**step)\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "        model_path: str = 'jhgan/ko-sroberta-multitask',         \n",
    "        device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = SentenceTransformer(model_path)\n",
    "        self.model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.5),\n",
    "#             nn.Linear(768, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 256)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def split_list(self, tokens: list, seq_len: int = 128, pad_idx=0):\n",
    "        batch = []\n",
    "\n",
    "        for i in range(int(len(tokens) // seq_len) + 1):\n",
    "            token = tokens[i*seq_len:(i+1)*seq_len]\n",
    "            if len(token) < 128:\n",
    "                token += [pad_idx] * (128 - len(token))\n",
    "\n",
    "            batch.append(token)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def make_batchs(\n",
    "        self,\n",
    "        tokens: dict,\n",
    "        seq_len: int = 128,\n",
    "        pad_idx: int = 0,\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    ):\n",
    "        return {\n",
    "            k: torch.tensor(self.split_list(v), device=device)\n",
    "            for k, v in tokens.items()\n",
    "        }\n",
    "    \n",
    "    def forward(self, x: str | list):\n",
    "        if isinstance(x, str):\n",
    "            x = [x]\n",
    "            \n",
    "        embeddings = []\n",
    "        for corpus in x:\n",
    "            t = self.model.tokenizer(corpus)\n",
    "            embedding = self.model(self.make_batchs(t, device=self.device))\n",
    "            embeddings.append(embedding['sentence_embedding'].mean(axis=0))\n",
    "            \n",
    "        y = torch.vstack(embeddings)\n",
    "        y = self.linear(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "class FakeReviewClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=14,\n",
    "        num_feature_layers=3,\n",
    "        feature_step=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature = FeatureClassifier(\n",
    "            input_size=input_size,\n",
    "            num_layers=num_feature_layers,\n",
    "            step=feature_step\n",
    "        )\n",
    "        \n",
    "        self.embedding = BertEmbedding()\n",
    "        \n",
    "        feature_output_size = self.feature.model[-1].out_features\n",
    "        embedding_output_size = self.embedding.linear[0].out_features\n",
    "        \n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(feature_output_size + embedding_output_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32, 8),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(8, 1),\n",
    "        )\n",
    "        \n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        feature_emb = self.feature(kwargs['feature'])\n",
    "        embedding_emb = self.embedding(kwargs['embedding'])\n",
    "                \n",
    "        con = torch.concat([feature_emb, embedding_emb], axis=1)\n",
    "        y = self.generator(con)\n",
    "        return self.activation(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dab55438",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FakeReviewClassifier(\n",
    "    num_feature_layers=2,\n",
    "    feature_step=1\n",
    ")\n",
    "crit = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b28e892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "early_stop = 30\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90f6038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CustomLoader(train_ds, batch_size=batch_size)\n",
    "valid_loader = CustomLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7f4ed90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FakeReviewClassifier(\n",
       "  (feature): FeatureClassifier(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=14, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=16, out_features=32, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (embedding): BertEmbedding(\n",
       "    (model): SentenceTransformer(\n",
       "      (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
       "      (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (generator): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=256, bias=True)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c36c04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 1000/1000 [11:41<00:00,  1.42it/s]\n",
      "100%|██████████| 251/251 [00:58<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss=0.08354, valid_loss=0.08153, valid_acc=0.625, best_acc=0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:43<00:00,  1.42it/s]\n",
      "100%|██████████| 251/251 [00:58<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.08277, valid_loss=0.08069, valid_acc=0.663, best_acc=0.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:43<00:00,  1.42it/s]\n",
      "100%|██████████| 251/251 [00:58<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss=0.08196, valid_loss=0.08223, valid_acc=0.6225, best_acc=0.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:43<00:00,  1.42it/s]\n",
      "100%|██████████| 251/251 [00:58<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss=0.08172, valid_loss=0.08027, valid_acc=0.6635, best_acc=0.6635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:44<00:00,  1.42it/s]\n",
      "100%|██████████| 251/251 [00:58<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_loss=0.0816, valid_loss=0.07997, valid_acc=0.6515, best_acc=0.6635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 346/1000 [04:04<06:46,  1.61it/s]"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "lowest_loss = np.inf\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for t_i in tqdm(range(int(len(train_ds) // batch_size) + 1)):\n",
    "        batch = train_loader.get_batch(t_i)\n",
    "        for k, v in batch.items():\n",
    "            if k != 'embedding':\n",
    "                batch[k] = v.to(device)\n",
    "            else:\n",
    "                batch[k] = v\n",
    "\n",
    "        if len(batch['feature']) == 0: continue\n",
    "        y_hat_i = model(**batch)\n",
    "        y_i = batch['label']\n",
    "\n",
    "        loss = crit(y_hat_i.squeeze(), y_i.squeeze())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += float(loss)\n",
    "\n",
    "    train_loss = train_loss / len(train_ds)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        y_hat = []\n",
    "        y_real = []\n",
    "        \n",
    "        for v_i in tqdm(range(int(len(valid_ds) // batch_size) + 1)):\n",
    "            batch = valid_loader.get_batch(v_i)\n",
    "            for k, v in batch.items():\n",
    "                if k != 'embedding':\n",
    "                    batch[k] = v.to(device)\n",
    "                else:\n",
    "                    batch[k] = v\n",
    "            \n",
    "            if len(batch['feature']) == 0: continue\n",
    "            y_hat_i = model(**batch)\n",
    "            y_i = batch['label']            \n",
    "            \n",
    "            y_hat += (y_hat_i.squeeze() > 0.5).cpu().numpy().astype(int).tolist()\n",
    "            y_real += y_i.tolist()\n",
    "            loss = crit(y_hat_i.squeeze(), y_i.squeeze())\n",
    "        \n",
    "            valid_loss += float(loss)\n",
    "            \n",
    "    valid_loss = valid_loss /len(valid_ds)\n",
    "    valid_acc = accuracy_score(y_real, y_hat)\n",
    "    \n",
    "    if valid_acc >= best_acc:\n",
    "        best_model = deepcopy(model.state_dict())\n",
    "        best_acc = valid_acc\n",
    "        best_epoch = epoch\n",
    "        \n",
    "    else:\n",
    "        if (epoch - best_epoch) >= early_stop:\n",
    "            print(\"Early Stop at Epoch {epoch}. Lowest Loss {lowest_loss}, Best Acc {best_acc} at {best_epoch} epoch\".format(\n",
    "                epoch=epoch,\n",
    "                lowest_loss=round(lowest_loss, 5),\n",
    "                best_acc=best_acc,\n",
    "                best_epoch=best_epoch\n",
    "            ))\n",
    "            break\n",
    "\n",
    "    print(\"Epoch {epoch}: train_loss={train_loss}, valid_loss={valid_loss}, valid_acc={valid_acc}, best_acc={best_acc}\".format(\n",
    "        epoch=epoch,\n",
    "        train_loss=round(train_loss, 5),\n",
    "        valid_loss=round(valid_loss, 5),\n",
    "        valid_acc=accuracy_score(y_real, y_hat),\n",
    "        best_acc=best_acc\n",
    "    ))\n",
    "\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "629a885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './model/multi_update_bert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719afe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bd8d37a",
   "metadata": {},
   "source": [
    "## Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17662633",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'jhgan/ko-sroberta-multitask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3954eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "043f7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6462f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/review_dataset_v6.csv', index_col=0)\n",
    "uid = df['id']\n",
    "X = df.content.tolist()\n",
    "y = df.label_f2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb6dabb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(tokens: list, seq_len: int = 128, pad_idx=0):\n",
    "    batch = []\n",
    "    \n",
    "    for i in range(int(len(tokens) // seq_len) + 1):\n",
    "        token = tokens[i*seq_len:(i+1)*seq_len]\n",
    "        if len(token) < 128:\n",
    "            token += [pad_idx] * (128 - len(token))\n",
    "        \n",
    "        batch.append(token)\n",
    "        \n",
    "    return batch\n",
    "    \n",
    "\n",
    "def make_batchs(\n",
    "    tokens: dict,\n",
    "    seq_len: int = 128,\n",
    "    pad_idx: int = 0,\n",
    "    to_tensor: str = 'pt',\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    if to_tensor:\n",
    "        return {\n",
    "            k: torch.tensor(split_list(v), device=device)\n",
    "            for k, v in tokens.items()\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        k: split_list(v)\n",
    "        for k, v in tokens.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a1194f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4f35641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10001it [05:10, 32.26it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_result = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, c in tqdm(zip(uid, X)):\n",
    "        t = model.tokenizer(c)\n",
    "        embedding = model.forward(make_batchs(t, device=device))\n",
    "        \n",
    "        embedding_result[i] = embedding['sentence_embedding'].mean(axis=0).cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dde3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(embedding_result).T\n",
    "df.columns = [f\"e_sum_{i}\" for i in range(768)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e2e63e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index().to_csv('./data/review_dataset_embedding_all_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6244db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
